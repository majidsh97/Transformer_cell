{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10, 512])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchsummary\n",
    "#https://storrs.io/attention/\n",
    "device = 'cpu'\n",
    "class transformer_cell(torch.nn.Module):\n",
    "    \"\"\"\n",
    " Q(batch_size, num_queries, embedding_dim)\n",
    " K(batch_size, num_keys, embedding_dim)\n",
    " V(batch_size, num_values, value_dim)\n",
    "\n",
    " num_queries, num_keys, and num_values are all equal to the sequence length.\n",
    " each ele\n",
    "\n",
    "\"\"\"\n",
    "    def __init__(self, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        d_emb = 512\n",
    "        d_ff = 100\n",
    "        self.atten = torch.nn.MultiheadAttention(d_emb,8)\n",
    "        self.ff = torch.nn.Sequential(\n",
    "            torch.nn.Linear(d_emb,d_ff),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(d_ff,d_emb)\n",
    "        )\n",
    "            \n",
    "        self.ln = torch.nn.LayerNorm(d_emb)\n",
    "        self.ln2 = torch.nn.LayerNorm(d_emb)\n",
    "\n",
    "    def forward(self, input,mask=None) -> torch.Tensor:\n",
    "        o = self.atten.forward(input,input,input,attn_mask=mask)[0]\n",
    "        o = o + self.ln.forward(o)\n",
    "\n",
    "        o = self.ff.forward(o)\n",
    "        o = o+ self.ln2(o)\n",
    "        return o\n",
    "        pass\n",
    "\n",
    "tc = transformer_cell().to(device)\n",
    "\n",
    "x = torch.normal(0,1,(1,10,512))\n",
    "o = tc.forward(x)\n",
    "print(o.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sure, let's consider a sequence of numbers as an example. Let's say we have the following sequence:\n",
    "\n",
    "`[1, 2, 3, 4, 5]`\n",
    "\n",
    "In a self-attention mechanism, this sequence would be transformed into a set of query, key, and value vectors using linear transformations (i.e., weight matrices and bias vectors). For simplicity, let's assume that these transformations result in the following query, key, and value vectors:\n",
    "\n",
    "```\n",
    "Q = [q1, q2, q3, q4, q5]\n",
    "K = [k1, k2, k3, k4, k5]\n",
    "V = [v1, v2, v3, v4, v5]\n",
    "```\n",
    "\n",
    "The self-attention mechanism computes the attention weights by taking the dot product of each query vector with each key vector, applying a softmax function to get probabilities, and then taking a weighted sum of the value vectors. For example, the first element in the output sequence would be computed as:\n",
    "\n",
    "```\n",
    "output1 = softmax(q1 * k1) * v1 + softmax(q1 * k2) * v2 + softmax(q1 * k3) * v3 + softmax(q1 * k4) * v4 + softmax(q1 * k5) * v5\n",
    "```\n",
    "\n",
    "This is repeated for each element in the sequence, resulting in a new sequence of the same length where each element is a weighted sum of all the elements in the original sequence.\n",
    "\n",
    "In this way, each element in the output sequence \"attends\" to all the other elements in the input sequence, allowing the model to capture complex relationships between different parts of the sequence.\n",
    "\n",
    "Let's assume that `Q`, `K`, and `V` are matrices, where each row corresponds to a vector in the query, key, and value sets, respectively. For example:\n",
    "\n",
    "```\n",
    "Q = [q1, q2, q3, q4, q5]\n",
    "K = [k1, k2, k3, k4, k5]\n",
    "V = [v1, v2, v3, v4, v5]\n",
    "```\n",
    "\n",
    "We can compute the attention weights using the matrix multiplication of `Q` and `K^T` (the transpose of `K`), followed by a softmax operation. This will give us a matrix `A` of attention weights, where each row corresponds to a weight vector for a single query vector. For example:\n",
    "\n",
    "```\n",
    "A = softmax(Q @ K.T)\n",
    "```\n",
    "\n",
    "Here, `@` denotes matrix multiplication.\n",
    "\n",
    "We can then compute the output matrix `O` by taking the matrix product of `A` and `V`:\n",
    "\n",
    "```\n",
    "O = A @ V\n",
    "```\n",
    "\n",
    "The first row of `O` will correspond to the first element in the output sequence, which is the weighted sum of all the value vectors, where the weights are given by the attention weights in the first row of `A`. In other words:\n",
    "\n",
    "```\n",
    "output1 = O[0, :]\n",
    "```\n",
    "\n",
    "This gives us the first element in the output sequence, computed using matrix multiplication.\n",
    "\n",
    "\n",
    "In the example I provided, the weights are implicitly included in the query, key, and value vectors. These vectors are typically obtained by applying linear transformations (i.e., weight matrices and bias vectors) to the input sequence.\n",
    "\n",
    "Let's denote the input sequence as `X`, and the weight matrices for the query, key, and value transformations as `W_q`, `W_k`, and `W_v`, respectively. Then, the query, key, and value vectors are computed as:\n",
    "\n",
    "```\n",
    "Q = X * W_q + b_q\n",
    "K = X * W_k + b_k\n",
    "V = X * W_v + b_v\n",
    "```\n",
    "\n",
    "Here, `b_q`, `b_k`, and `b_v` are bias vectors. The weight matrices and bias vectors are learnable parameters that are optimized during training to minimize the loss function of the model.\n",
    "\n",
    "In the example I provided, I simplified the explanation by assuming that the query, key, and value vectors were already given. In practice, these vectors are computed from the input sequence using the weight matrices and bias vectors."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
